{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WnI_LE7JMvqCFujUHYXoZd2jyk3jrU6-","timestamp":1663054922703},{"file_id":"1dvGxOyk9yrQChlNeFP1wKqdYQSILNcVy","timestamp":1662565253228},{"file_id":"1RJGkAoyVECEJzADu_DyRt9Eo6-UIeD3s","timestamp":1662564783382},{"file_id":"1OoOm8lwFIBNsFsd806mLfULxuNmHM-AT","timestamp":1662282022331},{"file_id":"1oXxBcgjEudkgwu9jM-ZDJPO9xnrUQmJq","timestamp":1662129279855},{"file_id":"19U9qHPY3Y19eqxsU4Fkx7r-hFVujSW0r","timestamp":1661937526517},{"file_id":"1w8WBc8uM9DSUZ3B8VRxPoXjGT3lsqoBR","timestamp":1659521981321}],"collapsed_sections":[],"mount_file_id":"1WnI_LE7JMvqCFujUHYXoZd2jyk3jrU6-","authorship_tag":"ABX9TyOxRbFBlsR/LLsVJtVVLQCP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YdWaDifi08qy","executionInfo":{"status":"ok","timestamp":1663055459631,"user_tz":-120,"elapsed":26821,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}},"outputId":"38c9dbcd-e79b-4637-9107-3d7ccb6a31b8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"wp2lRDVUr79L"}},{"cell_type":"code","source":["import copy\n","import csv\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import re\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import json\n","\n","from collections import Counter\n","from os import listdir\n","from os.path import isfile, join\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.models import resnet152"],"metadata":{"id":"0meW2Nk9IUtI","executionInfo":{"status":"ok","timestamp":1663055463187,"user_tz":-120,"elapsed":3570,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Images"],"metadata":{"id":"mtQczdZbr024"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"6fz5_Y_iSeYG","executionInfo":{"status":"ok","timestamp":1663055463188,"user_tz":-120,"elapsed":9,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"outputs":[],"source":["class Dataset(Dataset):\n","    def __init__(self, img_dir, transform=None):\n","        super(Dataset, self).__init__()\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.image_names = [image for image in listdir(self.img_dir) if isfile(join(self.img_dir, image))]\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","    \n","    def __getitem__(self, idx):\n","        img_path = self.image_names[idx]\n","        img = Image.open(join(self.img_dir, img_path)).convert('RGB')\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        return (img_path, img)\n","\n","\n","class ResNet(torch.nn.Module):\n","    \"\"\" ResNet is used to obtain image features. \"\"\"  \n","    def __init__(self):\n","        super(ResNet, self).__init__()\n","        self.model = resnet152()\n","        self.model.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=1)\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Mwuy_L5GzGij","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663055502443,"user_tz":-120,"elapsed":39262,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}},"outputId":"c4b840ca-13f4-4663-f2f0-7e40522988a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([6, 1000])\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019980.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019981.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019982.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019983.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019984.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019985.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019986.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019987.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019988.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015/abstract_v002_train2015_000000019989.npy\n"]}],"source":["src_dir_train = '/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_abstract_v002_train2015'\n","dst_dir_train = '/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015'\n","\n","resize_dim = 448\n","batch_size = 6\n","num_workers = 2\n","\n","transform = transforms.Compose([transforms.Resize((resize_dim, resize_dim)), transforms.ToTensor()])\n","\n","dataset_train = Dataset(src_dir_train, transform=transform)\n","\n","loader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n","\n","if not os.path.exists(dst_dir_train):\n","    os.makedirs(dst_dir_train)\n","  \n","model = ResNet()\n","\n","for i, (img_paths, images) in enumerate(loader_train):\n","    output = model(images)\n","    if i == 0:\n","        print(output.shape)\n","\n","    for j in range(len(img_paths)):\n","        feat_name = img_paths[j].replace('.png', '.npy')\n","        feat_name = join(dst_dir_train, feat_name)\n","        print(feat_name)\n","        np.save(feat_name, output[j].data.numpy())"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6rQ4vWQPF7C-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663055526975,"user_tz":-120,"elapsed":24541,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}},"outputId":"e6562123-fbb2-462d-a3f1-57e000c93633"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1000])\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_val2015/abstract_v002_val2015_000000029995.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_val2015/abstract_v002_val2015_000000029996.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_val2015/abstract_v002_val2015_000000029998.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_val2015/abstract_v002_val2015_000000029999.npy\n","/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_val2015/abstract_v002_val2015_000000029997.npy\n"]}],"source":["src_dir_val = '/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_abstract_v002_val2015'\n","dst_dir_val = '/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_val2015'\n","\n","dataset_val = Dataset(src_dir_val, transform=transform)\n","\n","loader_val = DataLoader(dataset_val, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n","\n","if not os.path.exists(dst_dir_val):\n","    os.makedirs(dst_dir_val)\n","\n","for i, (img_paths, images) in enumerate(loader_val):\n","    output = model(images)\n","    if i == 0:\n","        print(output.shape)\n","\n","    for j in range(len(img_paths)):\n","        feat_name = img_paths[j].replace('.png', '.npy')\n","        feat_name = join(dst_dir_val, feat_name)\n","        print(feat_name)\n","        np.save(feat_name, output[j].data.numpy())"]},{"cell_type":"markdown","source":["# Questions and Annotations"],"metadata":{"id":"1JDFGhrXrdMf"}},{"cell_type":"code","source":["# Interface for accessing the VQA dataset.\n","\n","# This code is based on the code available at the following link: https://github.com/GT-Vision-Lab/VQA/blob/master/PythonHelperTools/vqaTools/vqa.py.\n","\n","\n","class VQA:\n","\n","    def __init__(self, annotation_file=None, question_file=None):\n","        \"\"\" Constructor of VQA class for reading and visualizing questions and answers. \"\"\"\n","        # load dataset\n","        self.dataset = {}\n","        self.questions = {}\n","        self.qa = {}\n","        self.qqa = {}\n","        self.imgToQA = {}\n","        if annotation_file is not None and question_file is not None:\n","            dataset = json.load(open(annotation_file, 'r'))\n","            questions = json.load(open(question_file, 'r'))\n","            self.dataset = dataset\n","            self.questions = questions\n","            self.createIndex()\n","\n","    def createIndex(self):\n","\n","        # create index\n","        imgToQA = {ann['image_id']: [] for ann in self.dataset['annotations']}\n","        qa = {ann['question_id']: [] for ann in self.dataset['annotations']}\n","        qqa = {ann['question_id']: [] for ann in self.dataset['annotations']}\n","        for ann in self.dataset['annotations']:\n","            imgToQA[ann['image_id']] += [ann]\n","            qa[ann['question_id']] = ann\n","        for ques in self.questions['questions']:\n","            qqa[ques['question_id']] = ques\n","\n","        # create class members\n","        self.qa = qa\n","        self.qqa = qqa\n","        self.imgToQA = imgToQA"],"metadata":{"id":"og7oNHiFmeyd","executionInfo":{"status":"ok","timestamp":1663055526977,"user_tz":-120,"elapsed":41,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# This code is based on the code available at the following link: https://github.com/GT-Vision-Lab/VQA/blob/master/PythonEvaluationTools/vqaEvaluation/vqaEval.py\n","\n","\n","contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\",\n","                \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\",\n","                \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\",\n","                \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\",\n","                \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\",\n","                \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n","                \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n","                \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\",\n","                \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\",\n","                \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\",\n","                \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\",\n","                \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\",\n","                \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\",\n","                \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\",\n","                \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\",\n","                \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\",\n","                \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\",\n","                \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n","                \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\",\n","                \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\",\n","                \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\",\n","                \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n","\n","manualMap = {'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\n","\n","articles = ['a', 'an', 'the']\n","\n","\n","def processDigitArticle(inText):\n","\toutText = []\n","\ttempText = inText.lower().split()  # Making all characters lowercase.\n","\tfor word in tempText:\n","\t\tword = manualMap.setdefault(word, word)  # Converting number words to digits.\n","\t\t# The setdefault() method returns the value of the item with the specified key. If the key does not exist, insert the key, with the specified value.\n","\t\tif word not in articles:\n","\t\t\toutText.append(word)  # Removing articles (a, an, the).\n","\t\telse:\n","\t\t\tpass\n","\tfor wordId, word in enumerate(outText):\n","\t\tif word in contractions:\n","\t\t\toutText[wordId] = contractions[word]  # Adding apostrophe if a contraction is missing it.\n","\toutText = ' '.join(outText)\n","\t# The join() method takes all items in an iterable and joins them into one string. ' ' is the separator.\n","\treturn outText\n","\n","\n","periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n","commaStrip = re.compile(\"(\\d)(\\,)(\\d)\")\n","punct = [';', r\"/\", '[', ']', '\"', '{', '}', '(', ')', '=', '+', '\\\\', '_', '-', '>', '<', '@', '`', ',', '?', '!']  # r means the string will be treated as raw string.\n","\n","\n","def processPunctuation(inText):\n","\toutText = inText\n","\tfor p in punct:  # Replacing all punctuation (except apostrophe and colon) with a space character.\n","\t\tif (p + ' ' in inText or ' ' + p in inText) or (re.search(commaStrip, inText) is not None):  # In case of comma, no space is inserted if it occurs between digits.\n","\t\t\toutText = outText.replace(p, '')\n","\t\telse:\n","\t\t\toutText = outText.replace(p, ' ')\n","\toutText = periodStrip.sub(\"\", outText, re.UNICODE)  # Removing periods except if it occurs as decimal.\n","\treturn outText"],"metadata":{"id":"EWmqKSzv-wxS","executionInfo":{"status":"ok","timestamp":1663055526978,"user_tz":-120,"elapsed":39,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# This code is inspired by the code available at the following link: https://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py.\n","\n","\n","class VqaTrainDataset(Dataset):\n","\n","    \"\"\" Load the VQA dataset using the VQA class. \"\"\"\n","\n","    def __init__(self, question_json_file_path, annotation_json_file_path, image_filename_pattern, img_features_dir, vocab_json_filename):\n","          \n","        \"\"\"\n","        Args:\n","            question_json_file_path (string): Path to the json file containing the questions\n","            annotation_json_file_path (string): Path to the json file containing the annotations\n","            image_filename_pattern (string): Pattern used by the filenames of the images in this dataset (eg \"abstract_v002_train2015_{}.png\")\n","            img_features_dir (string): Path to the directory with image features\n","            vocab_json_filename (string): Path to the vocabulary.\n","        \"\"\"\n","\n","        vqa_db = VQA(annotation_file=annotation_json_file_path, question_file=question_json_file_path)\n","\n","        self.max_words_in_ques = -1  \n","        self.dataset = []\n","\n","        ques_list = []\n","        ans_list = []\n","\n","        for q_id, annotation in vqa_db.qa.items():\n","            entry = {}\n","            question = vqa_db.qqa[q_id]['question']\n","            question = processPunctuation(question)\n","            question = processDigitArticle(question)\n","            words = question.split(' ')\n","            if len(words) > self.max_words_in_ques:\n","                self.max_words_in_ques = len(words)\n","            ques_list += words\n","            entry['ques'] = words\n","            answer_objs = annotation['answers']\n","\n","            possible_answers = [a['answer'] for a in answer_objs]\n","\n","            entry['possible_answers'] = []\n","            for answer in possible_answers:\n","                mod_ans = processPunctuation(answer)\n","                mod_ans = processDigitArticle(mod_ans)\n","                ans_list.append(mod_ans)\n","                entry['possible_answers'].append(mod_ans)\n","      \n","            img_full_idx = \"%012d\" % annotation['image_id']  # '00000000000image_id'\n","            img_name = image_filename_pattern.replace('{}', img_full_idx)\n","            img_feature_loc = os.path.join(img_features_dir, img_name.replace('.png', '.npy'))\n","            entry['img_feat_loc'] = img_feature_loc\n","\n","            self.dataset.append(entry)\n","\n","        q_vocab = self.build_vocab(ques_list)\n","        a_vocab = self.build_vocab(ans_list)\n","        vocab = {'q': q_vocab, 'a': a_vocab}\n","\n","        f = open(vocab_path, \"w\")\n","        json.dump(vocab, f)\n","        f.close()\n","        \n","        self.q_vocab = vocab['q']\n","        self.q_vocab_size = len(self.q_vocab.keys())\n","            \n","        self.a_vocab = vocab['a']\n","        self.a_vocab_size = len(self.a_vocab.keys())\n","\n","    def build_vocab(self, data):\n","        counter = Counter(data)\n","        words = counter.keys()\n","        tokens = sorted(words, key=lambda x: (counter[x], x), reverse=True)  # reverse=True: sorts in a descending order.\n","        vocab = {t: i for i, t in enumerate(tokens)}\n","        return vocab\n","\n","    def _get_q_encoding(self, questions):\n","        vec = torch.zeros(self.q_vocab_size)\n","        for question in questions:\n","            if question in self.q_vocab:\n","                vec[self.q_vocab[question]] += 1\n","        return vec, len(questions)\n","\n","    def _get_a_encoding(self, answers):\n","        vec = torch.zeros(self.a_vocab_size)\n","        for answer in answers:\n","            if answer in self.a_vocab:\n","                vec[self.a_vocab[answer]] += 1\n","        return vec\n","\n","    def __getitem__(self, idx):\n","        entry = self.dataset[idx]\n","\n","        image_encoding = np.load(entry['img_feat_loc'])\n","\n","        ques = entry['ques']\n","        ques_encoding, ques_len = self._get_q_encoding(ques)\n","\n","        possible_answers = entry['possible_answers']\n","        ans_encoding = self._get_a_encoding(possible_answers)\n","\n","        return {'image_enc': image_encoding, 'ques_enc': ques_encoding, 'ques_len': ques_len, 'ans_enc': ans_encoding}\n","\n","    def __len__(self):\n","        return len(self.dataset)"],"metadata":{"id":"X97alyRomP7p","executionInfo":{"status":"ok","timestamp":1663055526980,"user_tz":-120,"elapsed":39,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# This code is inspired by the code available at the following link: https://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py.\n","\n","\n","class VqaValDataset(Dataset):\n","\n","    \"\"\"\n","    Load the VQA dataset using the VQA class.\n","    \"\"\"\n","\n","    def __init__(self, question_json_file_path, annotation_json_file_path, image_filename_pattern, img_features_dir, vocab_json_filename):\n","                 \n","        \"\"\"\n","        Args:\n","            question_json_file_path (string): Path to the json file containing the questions\n","            annotation_json_file_path (string): Path to the json file containing the annotations\n","            image_filename_pattern (string): Pattern used by the filenames of the images in this dataset (eg \"abstract_v002_val2015_{}.png\")\n","            img_features_dir (string): Path to the directory with image features\n","            vocab_json_filename (string): Path to the vocabulary.\n","        \"\"\"\n","\n","        vqa_db = VQA(annotation_file=annotation_json_file_path, question_file=question_json_file_path)\n","\n","        self.max_words_in_ques = -1\n","            \n","        self.dataset = []\n","\n","        for q_id, annotation in vqa_db.qa.items():\n","            entry = {}\n","            question = vqa_db.qqa[q_id]['question']\n","            question = processPunctuation(question)\n","            question = processDigitArticle(question)\n","            words = question.split(' ')\n","            if len(words) > self.max_words_in_ques:\n","                self.max_words_in_ques = len(words)\n","            entry['ques'] = words\n","            answer_objs = annotation['answers']\n","\n","            possible_answers = [a['answer'] for a in answer_objs]\n","\n","            entry['possible_answers'] = []\n","            for answer in possible_answers:\n","                mod_ans = processPunctuation(answer)\n","                mod_ans = processDigitArticle(mod_ans)\n","                entry['possible_answers'].append(mod_ans)\n","                \n","            img_full_idx = \"%012d\" % annotation['image_id']  # '00000000000image_id'\n","            img_name = image_filename_pattern.replace('{}', img_full_idx)\n","            img_feature_loc = os.path.join(img_features_dir, img_name.replace('.png', '.npy'))\n","            entry['img_feat_loc'] = img_feature_loc\n","            \n","            self.dataset.append(entry)\n","\n","        vocab = json.load(open(vocab_json_filename, \"r\"))\n","\n","        self.q_vocab = vocab['q']\n","        self.q_vocab_size = len(self.q_vocab.keys())\n","            \n","        self.a_vocab = vocab['a']\n","        self.a_vocab_size = len(self.a_vocab.keys())\n","\n","    def _get_q_encoding(self, questions):\n","        vec = torch.zeros(self.q_vocab_size)\n","        for question in questions:\n","            if question in self.q_vocab:\n","                vec[self.q_vocab[question]] += 1\n","        return vec, len(questions)\n","\n","    def _get_a_encoding(self, answers):\n","        vec = torch.zeros(self.a_vocab_size)\n","        for answer in answers:\n","            if answer in self.a_vocab:\n","                vec[self.a_vocab[answer]] += 1\n","        return vec\n","\n","    def __getitem__(self, idx):\n","        entry = self.dataset[idx]\n","\n","        image_encoding = np.load(entry['img_feat_loc'])\n","\n","        ques = entry['ques']\n","        ques_encoding, ques_len = self._get_q_encoding(ques)\n","\n","        possible_answers = entry['possible_answers']\n","        ans_encoding = self._get_a_encoding(possible_answers)\n","\n","        return {'image_enc': image_encoding, 'ques_enc': ques_encoding, 'ques_len': ques_len, 'ans_enc': ans_encoding}\n","\n","    def __len__(self):\n","        return len(self.dataset)"],"metadata":{"id":"K0rLM2xhNDYP","executionInfo":{"status":"ok","timestamp":1663055526981,"user_tz":-120,"elapsed":39,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"VFrpkrT7qJOF"}},{"cell_type":"markdown","source":["## SimpleBaselineNet"],"metadata":{"id":"Rjg3rh0moUII"}},{"cell_type":"code","source":["class SimpleBaselineNet(nn.Module):\n","\n","    \"\"\" Predicts an answer to a question about an image using the Simple Baseline for Visual Question Answering (Zhou et al, 2017) paper. \"\"\"\n","\n","    \"\"\" \n","      The input question is first converted to a one-hot vector, which is transformed to a word feature via a word embedding layer and then is concatenated with the image feature from CNN.\n","      The combined feature is sent to the softmax layer to predict the answer class.\n","      This model consists of only two linear transformations from the one hot vector to the answer response:\n","        - one is the word embedding;\n","        - the other is the softmax matrix multiplication. \n","    \"\"\"\n","\n","    def __init__(self, img_feat_size, q_vocab_size, a_vocab_size):\n","        super().__init__()\n","\n","        self.img_feat_size = img_feat_size\n","        self.q_vocab_size = q_vocab_size\n","        self.a_vocab_size = a_vocab_size\n","        self.q_embedding_size = 1024\n","\n","        self.linear_layer = nn.Linear(self.q_vocab_size, self.q_embedding_size, bias=False)  # Applies a linear transformation to the incoming data: y = xA^T + b.\n","        self.classifier = nn.Linear(self.img_feat_size + self.q_embedding_size, self.a_vocab_size, bias=False)\n","\n","    def forward(self, image_encoding, question_encoding):\n","\n","        image_encoding = image_encoding.view(image_encoding.shape[0], -1)  # image_encoding.size(): torch.Size([24, 1000]).\n","\n","        question_embedding = self.linear_layer(question_encoding)\n","\n","        x = torch.cat((image_encoding, question_embedding), dim=-1)  # Concatenates the given sequence of tensors (image_encoding, question_embedding) in the given dimension (dim=-1).\n","\n","        out = self.classifier(x)\n","\n","        return out"],"metadata":{"id":"rtEjHwt3nlYs","executionInfo":{"status":"ok","timestamp":1663055526982,"user_tz":-120,"elapsed":39,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## CoattentionNet"],"metadata":{"id":"QoCLzk6HohzM"}},{"cell_type":"code","source":["class QuestionProcessor(nn.Module):\n","\n","    \"\"\"\n","      - At the word level, words are embedded into a vector space through an embedding matrix.\n","\n","      - At the phrase level, 1-dimensional convolution neural networks (CNN) are used to capture the information contained in unigrams, bigrams and trigrams, and then combine various n-gram responses by pooling \n","        them into a single phrase level representation. \n","\n","      - At the question level, recurrent neural networks (RNN) encode the entire question, exactly a LSTM encodes the sequence after a max-pooling.  \n","    \"\"\"\n","\n","    def __init__(self, q_vocab_size, img_feat_size):\n","        super().__init__()\n","        self.word_embedding = nn.Embedding(q_vocab_size, img_feat_size)\n","\n","        self.phrase_unigram = nn.Conv1d(img_feat_size, img_feat_size, kernel_size=1, stride=1, padding=0)\n","        self.phrase_bigram = nn.Conv1d(img_feat_size, img_feat_size, kernel_size=2, stride=1, padding=1, dilation=2)  # dilation controls the spacing between the kernel points.\n","        self.phrase_trigram = nn.Conv1d(img_feat_size, img_feat_size, kernel_size=3, stride=1, padding=2, dilation=2)\n","        \n","        self.max_pool = nn.MaxPool2d(1)\n","        self.activation = nn.Tanh()\n","        self.dropout = nn.Dropout(0.5)\n","\n","        self.lstm = nn.LSTM(input_size=img_feat_size, hidden_size=img_feat_size, num_layers=2, batch_first=True)  \n","        # batch_first=True: the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). \n","\n","    def forward(self, question_encoding, question_length):\n","        w_embed = self.word_embedding(question_encoding.long())  # long() converts a torch.FloatTensor to a torch.LongTensor.\n","        w_embed_permute = w_embed.permute(0, 2, 1)  # permute() returns a view of the original tensor with its dimensions permuted.\n","\n","        uni_embed = self.phrase_unigram(w_embed_permute)\n","        bi_embed = self.phrase_bigram(w_embed_permute)\n","        tri_embed = self.phrase_trigram(w_embed_permute)\n","\n","        p_embed = self.max_pool(torch.cat((uni_embed, bi_embed, tri_embed), dim=2))\n","        # torch.cat() concatenates the given sequence of tensors (uni_embed, bi_embed, tri_embed) in the given dimension (dim=2).\n","        p_embed = self.dropout(self.activation(p_embed.permute(0, 2, 1)))  \n","\n","        packed = nn.utils.rnn.pack_padded_sequence(p_embed, question_length, batch_first=True, enforce_sorted=False)  # nn.utils.rnn.pack_padded_sequence() packs a Tensor containing padded sequences of variable length.\n","        # enforce_sorted=False: the input will get sorted unconditionally.\n","        q_embed, (_, _) = self.lstm(packed)\n","        q_embed, _ = nn.utils.rnn.pad_packed_sequence(q_embed, batch_first=True)  # nn.utils.rnn.pad_packed_sequence() pads a packed batch of variable length sequences.\n","        # pad_packed_sequence() is an inverse operation to pack_padded_sequence().\n","\n","        return (w_embed, p_embed, q_embed)\n","\n","\n","class ParallelAttention(nn.Module):\n","\n","    \"\"\"\n","      The parallel co-attention model generates image attention and question attention simultaneously. \n","      It connects the image and the question by calculating the similarity between image and question features at all pairs of image-location and question-location.\n","      Given an image feature map V and a question representation Q, the affinity matrix C is calculated by: C = tanh(Q^T*Wb*V) where Wb contains weights.\n","      Considering this affinity matrix as a feature, the model learns to predict image and question attention maps via following equations: \n","          Hv = tanh(Wv*V + C(Wq*Q)), Hq = tanh(Wq*Q + C^T(Wv*V))\n","          av = softmax((whv)^T*Hv), aq = softmax((whq)^T*Hq)\n","      where Wv, Wq, whv, whq are weight parameters, while av and aq are attention probabilities of each image region v and word q respectively.\n","      Image and question attention vectors are calculated as the weighted sum of image and question features.\n","    \"\"\"\n","\n","    def __init__(self, img_feat_size, hidden_size):\n","        super().__init__()\n","\n","        self.img_feat_size = img_feat_size    \n","        self.hidden_size = hidden_size\n","        \n","        self.linear_1 = nn.Linear(self.img_feat_size, self.hidden_size, bias=False)\n","        self.linear_2 = nn.Linear(self.hidden_size, 1, bias=False)\n","\n","        self.activation = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, ques_embed, img_embed):\n","\n","        ques_embed_permute = ques_embed.permute(0, 2, 1)  # permute() returns a view of the original tensor with its dimensions permuted.\n","        img_embed_permute = img_embed.permute(0, 2, 1)\n","\n","        activation = self.activation(torch.matmul(self.linear_1(ques_embed), img_embed))  # matmul() returns the matrix product of two arrays.\n","        activation_permute = activation.permute(0, 2, 1)\n","\n","        img = self.activation(self.linear_1(img_embed_permute) + torch.matmul(activation_permute, self.linear_1(ques_embed)))\n","        ques = self.activation(self.linear_1(ques_embed) + torch.matmul(activation, self.linear_1(img_embed_permute)))\n","\n","        attention_img = self.softmax(self.linear_2(img).squeeze())  # squeeze() returns a tensor with all the dimensions of input of size 1 removed.\n","        attention_ques = self.softmax(self.linear_2(ques).squeeze()).unsqueeze(1)  # unsqueeze() returns a new tensor with a dimension of size one inserted at the specified position.\n","\n","        attention_img_feat = torch.sum(torch.mul(img_embed, attention_img), dim=-1)  # mul() returns the product of two variables.\n","        attention_ques_feat = torch.sum(torch.mul(ques_embed_permute, attention_ques), dim=-1)\n","\n","        return attention_img_feat, attention_ques_feat\n","\n","\n","class AlternateAttention(nn.Module):\n","  \n","    \"\"\"\n","      The alternating co-attention model sequentially alternates between generating image and question attentions.\n","      It summarizes the question into a single vector, then it attends to the image based on the question feature, and, in the end, it attends to the question based on the attended image feature. \n","      It defines an attention operation which takes image (or question) features X and an attention guidance g derived from question (or image) as inputs, and outputs the attended image (or question) vector. \n","      The operation can be expressed in following steps:\n","          H = tanh(Wx*X + (Wg*g)1^T)\n","          ax = softmax((whx)^T*H)\n","          x = sum(ax*x)\n","      where 1 is a vector with all elements equal to one. Wx, Wg and whx are parameters, while ax is the attention weight of the feature X.\n","    \"\"\"\n","\n","    def __init__(self, img_feat_size, hidden_size):\n","        super().__init__()\n","        \n","        self.img_feat_size = img_feat_size\n","        self.hidden_size = hidden_size \n","        \n","        self.linear_1 = nn.Linear(self.img_feat_size, self.hidden_size, bias=False)\n","        self.linear_2 = nn.Linear(self.hidden_size, 1, bias=False)\n","\n","        self.activation = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, ques_embed, img_embed):\n","        \n","        img_embed_permute = img_embed.permute(0, 2, 1)  # permute() returns a view of the original tensor with its dimensions permuted.\n","\n","        a = torch.mul(ques_embed, self.softmax(self.linear_2(self.activation(self.linear_1(ques_embed)))))  # mul() returns the product of two variables.\n","        a = torch.sum(a, dim=1).squeeze()  # squeeze() returns a tensor with all the dimensions of input of size 1 removed.\n","\n","        a = torch.mul(img_embed_permute, self.softmax(self.linear_2(self.activation(self.linear_1(img_embed_permute) + self.linear_1(a).unsqueeze(1)))))  # unsqueeze() returns a new tensor with a dimension of size one inserted at the specified position.\n","        attention_img_feat = torch.sum(a, dim=1).squeeze()\n","\n","        a = torch.mul(ques_embed, self.softmax(self.linear_2(self.activation(self.linear_1(ques_embed) + self.linear_1(attention_img_feat).unsqueeze(1)))))\n","        attention_ques_feat = torch.sum(a, dim=1).squeeze()\n","\n","        return attention_img_feat, attention_ques_feat\n","\n","\n","class CoattentionNet(nn.Module):\n","\n","    \"\"\" Predicts an answer to a question about an image using the Hierarchical Question-Image Co-Attention for Visual Question Answering (Lu et al, 2017) paper. \"\"\"\n","\n","    \"\"\"\n","      Builds a hierarchical architecture that co-attends to the image and to the question on three levels: word level, phrase level, question level.\n","      A Multi-Layer Perceptron (MLP) is used to recursively encode attention features. \n","      A MLP is a fully connected neural network classifier with 2 hidden layers with tanh non-linearity:\n","          hw = tanh(Ww(qw + vw))\n","          hp = tanh(Wp[(qp + vp), hw])\n","          hs = tanh(Ws[(qs + vs), hp])\n","      where Ww, Wp and Ws are weight parameters.\n","    \"\"\"\n","\n","    def __init__(self, img_feat_size, q_vocab_size, a_vocab_size):\n","        super().__init__()\n","\n","        self.img_feat_size = img_feat_size\n","        self.q_vocab_size = q_vocab_size\n","        self.a_vocab_size = a_vocab_size\n","        self.hidden_size = 1000\n","\n","        self.q_network = QuestionProcessor(self.q_vocab_size, self.img_feat_size)\n","        \n","        # self.attention = ParallelAttention(self.img_feat_size, self.hidden_size)\n","        self.attention = AlternateAttention(self.img_feat_size, self.hidden_size)\n","\n","        self.attention_word = nn.Linear(self.img_feat_size, self.img_feat_size, bias=False)\n","        self.attention_phrase = nn.Linear(2*self.img_feat_size, self.img_feat_size, bias=False)\n","        self.attention_question = nn.Linear(2*self.img_feat_size, self.img_feat_size, bias=False)\n","\n","        self.classifier = nn.Linear(self.img_feat_size, self.a_vocab_size, bias=False)\n","\n","        self.activation = nn.Tanh()\n","\n","    def forward(self, image_encoding, question_encoding, question_length):\n","        (w_embed, p_embed, q_embed) = self.q_network(question_encoding, question_length)\n","\n","        image_encoding_size = image_encoding.size()  # image_encoding.size(): torch.Size([24, 1000]).\n","        image_embed = image_encoding.view(image_encoding_size[0], image_encoding_size[1], -1)\n","\n","        img_feat_word, ques_feat_word = self.attention(w_embed, image_embed)\n","        img_feat_phrase, ques_feat_phrase = self.attention(p_embed, image_embed)\n","        img_feat_question, ques_feat_question = self.attention(q_embed, image_embed)\n","\n","        res_word = self.activation(self.attention_word(img_feat_word + ques_feat_word))\n","\n","        res_phrase = self.activation(self.attention_phrase(torch.cat([img_feat_phrase + ques_feat_phrase, res_word], dim=-1)))  \n","        # torch.cat() concatenates the given sequence of tensors (img_feat_phrase + ques_feat_phrase, res_word) in the given dimension (dim=-1).\n","\n","        res_question = self.activation(self.attention_question(torch.cat([img_feat_question + ques_feat_question, res_phrase], dim=-1)))\n","        # torch.cat() concatenates the given sequence of tensors (img_feat_question + ques_feat_question, res_phrase) in the given dimension (dim=-1).\n","\n","        out = self.classifier(res_question)\n","        return out"],"metadata":{"id":"Bf4hWvgIMvF9","executionInfo":{"status":"ok","timestamp":1663055526984,"user_tz":-120,"elapsed":40,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Transformer"],"metadata":{"id":"VNttemo9ouNx"}},{"cell_type":"code","source":["def attention(query, key, value, scaled_term, dropout):\n","\n","    \"\"\"\n","    Scale Dot-Product Attention operation: \n","    First, it computes the dot product of the query Q and the key K and divides it by the square root of the scaled term. \n","    Then, it applies a softmax function to get weights on values.\n","    At the end, it computes the dot product of the output of the softmax function and the value V.\n","    The attended feature F is obtained as follows:\n","        W = score(Q, K) = softmax((Q*transpose(K)) / square_root(scaled_term))\n","        F = attention(Q, K, V) = W*V\n","    where W is the weight matrix.\n","    \"\"\"\n","\n","    score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(scaled_term)  # matmul() returns the matrix product of two arrays.\n","    score = F.softmax(score, dim=-1)  # softmax() for scaling in range [0, 1].\n","    if dropout is not None:\n","        score = dropout(score)\n","    output = torch.matmul(score, value)  # matmul() returns the matrix product of two arrays.\n","    return output\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" The Multi-Head Attention includes num_heads parallel heads, and each head is equivalent to an independent Scale Dot-Product Attention operation. \"\"\"\n","    def __init__(self, num_heads, embedding_dim, dropout):\n","        super().__init__()\n","        \n","        self.num_heads = num_heads\n","        self.embedding_dim = embedding_dim\n","        self.scaled_term = self.embedding_dim // self.num_heads\n","        \n","        self.linear = nn.Linear(embedding_dim, embedding_dim) \n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, query, key, value):      \n","        query_size = query.size(0)\n","\n","        key = self.linear(key).view(query_size, -1, self.num_heads, self.scaled_term)  # view() returns a new tensor with the same data but of a different shape.\n","        query = self.linear(query).view(query_size, -1, self.num_heads, self.scaled_term)\n","        value = self.linear(value).view(query_size, -1, self.num_heads, self.scaled_term)\n","        \n","        key = key.transpose(1, 2)\n","        query = query.transpose(1, 2)\n","        value = value.transpose(1, 2)\n","        \n","        att = attention(query, key, value, self.scaled_term, self.dropout)\n","        att = att.transpose(1, 2)\n","\n","        output = self.linear(att.contiguous().view(query_size, -1, self.embedding_dim))  # contiguous() returns a contiguous in memory tensor containing the same data.\n","        return output\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    \"\"\" Feed Forward consists of two fully connected layers with ReLU function. Dropout is applied to prevent overfitting. \"\"\"    \n","    def __init__(self, embedding_dim, feed_forward_dim, dropout):\n","        super(PositionwiseFeedForward, self).__init__()\n","\n","        self.linear_1 = nn.Linear(embedding_dim, feed_forward_dim)\n","        self.linear_2 = nn.Linear(feed_forward_dim, embedding_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.linear_2(self.dropout(self.relu(self.linear_1(x))))\n","\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, embedding_dim, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.eps = eps\n","\n","        self.ones = nn.Parameter(torch.ones(embedding_dim))\n","        self.zeros = nn.Parameter(torch.zeros(embedding_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)  # keepdim=True: the output tensor has dim retained.\n","        std = x.std(-1, keepdim=True)\n","        return self.ones * (x - mean) / (std + self.eps) + self.zeros\n","\n","\n","def get_clones(module, N):\n","    \"\"\" Produces N identical layers. \"\"\"\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"],"metadata":{"id":"71EuiOiPj_DV","executionInfo":{"status":"ok","timestamp":1663055526985,"user_tz":-120,"elapsed":40,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class FeatureEmbedding(nn.Module):\n","    \"\"\" Projects image features into a space of dimensionality `embedding_dim`. \"\"\"\n","    def __init__(self, img_feat_size, embedding_dim):\n","        super().__init__()\n","        self.linear = nn.Linear(img_feat_size, embedding_dim)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","\n","class EncoderLayer(nn.Module):\n","    \"\"\" A single encoder layer consists of one Multi-Head Attention, Feed Forward, Layer Normalization and Dropout. \"\"\"\n","    def __init__(self, embedding_dim, feed_forward_dim, num_heads, dropout):\n","        super().__init__()\n","\n","        self.norm = LayerNorm(embedding_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.attn = MultiHeadAttention(num_heads, embedding_dim, dropout)\n","        self.ff = PositionwiseFeedForward(embedding_dim, feed_forward_dim, dropout)\n","        \n","    def forward(self, x):\n","        x2 = self.norm(x)\n","\n","        # Encoder self-attention\n","        x = x + self.dropout(self.attn(x2, x2, x2))\n","        x2 = self.norm(x)\n","\n","        x = x + self.dropout(self.ff(x2))\n","        return x\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\" Encoder with N-stacked EncoderLayers and a layer normalization. \"\"\"\n","    def __init__(self, img_feat_size, embedding_dim, feed_forward_dim, num_layers, num_heads, dropout):\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","        self.feature_embedding = FeatureEmbedding(img_feat_size, embedding_dim)\n","        self.layers = get_clones(EncoderLayer(embedding_dim, feed_forward_dim, num_heads, dropout), num_layers)\n","        self.norm = LayerNorm(embedding_dim)\n","\n","    def forward(self, x):\n","        for i in range(self.num_layers):\n","            x2 = self.layers[i](self.feature_embedding(x))\n","        return self.norm(x2)"],"metadata":{"id":"PGLAVQJBo4SC","executionInfo":{"status":"ok","timestamp":1663055526986,"user_tz":-120,"elapsed":40,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    \"\"\" A single decoder layer consists of two Multi-Head Attentions, Feed Forward, Layer Normalization and Dropout. \"\"\"\n","    def __init__(self, embedding_dim, feed_forward_dim, num_heads, dropout):\n","        super().__init__()\n","\n","        self.norm = LayerNorm(embedding_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.attn = MultiHeadAttention(num_heads, embedding_dim, dropout)\n","        self.ff = PositionwiseFeedForward(embedding_dim, feed_forward_dim, dropout)\n","\n","    def forward(self, x, encoder_outputs):\n","        x2 = self.norm(x)\n","\n","        # Decoder self-attention\n","        x = x + self.dropout(self.attn(x2, x2, x2))\n","        x2 = self.norm(x)\n","\n","        # Encoder Decoder attention\n","        x = x + self.dropout(self.attn(x2, encoder_outputs, encoder_outputs))\n","        x2 = self.norm(x)\n","\n","        x = x + self.dropout(self.ff(x2))\n","        return x\n","\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\" Positional Encoding adds additional positional information to Embeddings. \"\"\"\n","    def __init__(self, embedding_dim, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, embedding_dim)\n","        position = torch.arange(0, max_len).unsqueeze(1)  # torch.arange() returns a 1-D tensor with values from the interval [0, max_len).\n","        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.pe = pe.unsqueeze(0)  # unsqueeze() returns a new tensor with a dimension of size one inserted at the specified position.\n","        \n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)] \n","        return self.dropout(x)\n","\n","\n","class Embeddings(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(Embeddings, self).__init__()\n","\n","        self.embedding_dim = embedding_dim\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","    def forward(self, x): \n","        x = self.embedding(x.long())  # long() converts a torch.FloatTensor to a torch.LongTensor.\n","        return x\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\" Decoder with N-stacked DecoderLayers and a layer normalization. \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, feed_forward_dim, num_layers, num_heads, dropout):\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","        self.embed = Embeddings(vocab_size, embedding_dim)\n","        self.position = PositionalEncoding(embedding_dim, dropout)\n","        self.layers = get_clones(DecoderLayer(embedding_dim, feed_forward_dim, num_heads, dropout), num_layers)\n","        self.norm = LayerNorm(embedding_dim)\n","\n","    def forward(self, x, encoder_outputs):\n","        for i in range(self.num_layers):\n","            x2 = self.layers[i](self.position(self.embed(x)), encoder_outputs)\n","        return self.norm(x2)"],"metadata":{"id":"pUUGw9r7okrM","executionInfo":{"status":"ok","timestamp":1663055526987,"user_tz":-120,"elapsed":40,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    \"\"\"\n","      Includes an Encoder module, a Decoder module and a fully connected layer.\n","    \"\"\"\n","    def __init__(self, img_feat_size, vocab_size, num_classes, embedding_dim, feed_forward_dim, num_layers, num_heads, dropout):\n","        super().__init__()\n","        \n","        self.encoder = Encoder(img_feat_size, embedding_dim, feed_forward_dim, num_layers, num_heads, dropout)\n","        self.decoder = Decoder(vocab_size, embedding_dim, feed_forward_dim, num_layers, num_heads, dropout)\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, image_encoding, question_encoding):\n","        encoder_outputs = self.encoder(image_encoding)\n","        decoder_output = self.decoder(question_encoding, encoder_outputs)\n","\n","        output = self.classifier(decoder_output).mean(dim=1)\n","        return output\n","\n","\n","def get_transformer_model(img_feat_size, vocab_size, num_classes): \n","\n","    transformer_config = {\n","        'img_feat_size':     img_feat_size,\n","        'vocab_size':        vocab_size, \n","        'num_classes':       num_classes,\n","        \"embedding_dim\":     512,\n","        \"feed_forward_dim\":  2048,\n","        \"num_layers\":        6,\n","        \"num_heads\":         4,\n","        \"dropout\":           0.1,\n","    }\n","\n","    return Transformer(**transformer_config)"],"metadata":{"id":"fqNMgpJyoZ49","executionInfo":{"status":"ok","timestamp":1663055527500,"user_tz":-120,"elapsed":552,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Train and Validate "],"metadata":{"id":"UTh28ay3pA8N"}},{"cell_type":"code","source":["train_question_path = '/content/drive/MyDrive/EasyVisualQuestionAnswering/OpenEnded_abstract_train2015_questions.txt'\n","train_annotation_path = '/content/drive/MyDrive/EasyVisualQuestionAnswering/abstract_train2015_annotations.txt'\n","train_img_feat_path = '/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_train2015'\n","\n","val_question_path = '/content/drive/MyDrive/EasyVisualQuestionAnswering/OpenEnded_abstract_val2015_questions.txt'\n","val_annotation_path = '/content/drive/MyDrive/EasyVisualQuestionAnswering/abstract_val2015_annotations.txt'\n","val_img_feat_path = '/content/drive/MyDrive/EasyVisualQuestionAnswering/scene_img_feat_abstract_v002_val2015'\n","\n","vocab_path = '/content/drive/MyDrive/EasyVisualQuestionAnswering/vocab.json'\n","\n","# results = '/content/drive/MyDrive/EasyVisualQuestionAnswering/SimpleBaselineNet.txt'\n","# results = '/content/drive/MyDrive/EasyVisualQuestionAnswering/ParallelCoattentionNet.txt'\n","# results = '/content/drive/MyDrive/EasyVisualQuestionAnswering/AlternateCoattentionNet.txt'\n","results = '/content/drive/MyDrive/EasyVisualQuestionAnswering/EncoderDecoder.txt'\n","\n","num_epochs = 5\n","\n","batch_size = 6\n","num_data_loader_workers = 2\n","\n","img_feat_size = 1000  "],"metadata":{"id":"1LQg_4hXnv5p","executionInfo":{"status":"ok","timestamp":1663055547100,"user_tz":-120,"elapsed":10,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def optimize(criterion, predicted_answer, optimizer, ground_truth_answer):\n","\n","    majority_ans = torch.argmax(ground_truth_answer, dim=-1)\n","    loss = criterion(predicted_answer, majority_ans)  # loss = criterion(output_model, target)\n","    optimizer.zero_grad()  # zero_grad(): zeroes the grad attribute of all the parameters passed to the optimizer.\n","    loss.backward()  # backward(): performs the gradient of all the parameters for which require_grad = True and stores the gradient in parameter.grad attribute for every parameter.\n","    optimizer.step()  # step(): updates the value of all the parameters passed to the optimizer (based on parameter.grad).\n","    return loss\n","\n","\n","def validate(model, val_dataset_loader):\n","\n","    correct_answers = 0\n","    total_answers = 0\n","\n","    for batch_id, batch_data in enumerate(val_dataset_loader):\n","        image_encoding = batch_data['image_enc']\n","        question_encoding = batch_data['ques_enc']\n","        question_length = batch_data['ques_len']\n","        ground_truth_answer = batch_data['ans_enc']\n","\n","        batch_size = ground_truth_answer.shape[0]\n","\n","        logits = model(image_encoding, question_encoding)  # for SimpleBaselineNet and Transformer\n","        # logits = model(image_encoding, question_encoding, question_length)  # for CoattentionNet \n","\n","        probs = F.softmax(logits, dim=-1)\n","        predicted_answer = torch.argmax(probs, dim=-1)\n","\n","        counts = ground_truth_answer[torch.arange(batch_size), predicted_answer]  # torch.arange(): returns a 1-D tensor with values in the range [start, end) with start = 0 and end = batch_size.\n","        correct_answers = correct_answers + float(torch.sum(torch.min(counts/3, torch.ones(1))))\n","            \n","        total_answers = total_answers + batch_size\n","\n","    return (correct_answers / total_answers) * 100"],"metadata":{"id":"lFpaARiguv6n","executionInfo":{"status":"ok","timestamp":1663055527504,"user_tz":-120,"elapsed":20,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"id":"UofRKAAglppf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663055579649,"user_tz":-120,"elapsed":26814,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}},"outputId":"55b65082-79ed-4888-f5c0-4ed915d3b29b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 has val accuracy: 22.22222222222222\n","Epoch 1 has val accuracy: 0.0\n","Epoch 2 has val accuracy: 22.22222222222222\n","Epoch 3 has val accuracy: 22.22222222222222\n","Epoch 4 has val accuracy: 22.22222222222222\n"]}],"source":["train_dataset = VqaTrainDataset(question_json_file_path=train_question_path,\n","                                annotation_json_file_path=train_annotation_path,\n","                                image_filename_pattern=\"abstract_v002_train2015_{}.png\",\n","                                img_features_dir=train_img_feat_path,\n","                                vocab_json_filename=vocab_path)\n","\n","val_dataset = VqaValDataset(question_json_file_path=val_question_path,\n","                            annotation_json_file_path=val_annotation_path,\n","                            image_filename_pattern=\"abstract_v002_val2015_{}.png\",\n","                            img_features_dir=val_img_feat_path,\n","                            vocab_json_filename=vocab_path)\n","\n","\n","train_dataset_loader = DataLoader(train_dataset,\n","                                  batch_size=batch_size,  # batch_size: represents how many samples per batch to load.\n","                                  shuffle=True,\n","                                  num_workers=num_data_loader_workers)  # num_workers: represents how many subprocesses to use for loading data.\n","\n","val_dataset_loader = DataLoader(val_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False,\n","                                num_workers=num_data_loader_workers)\n","\n","\n","q_vocab_size = train_dataset.q_vocab_size\n","a_vocab_size = train_dataset.a_vocab_size\n","\n","# model = SimpleBaselineNet(img_feat_size, q_vocab_size, a_vocab_size)\n","# model = CoattentionNet(img_feat_size, q_vocab_size, a_vocab_size)\n","model = get_transformer_model(img_feat_size=img_feat_size, vocab_size=q_vocab_size, num_classes=a_vocab_size)\n","\n","out_filename = open(results, \"w\")\n","\n","for epoch in range(num_epochs):\n","    num_batches = len(train_dataset_loader)\n","\n","    for batch_id, batch_data in enumerate(train_dataset_loader):\n","        model.train()  # Set the model to train mode\n","\n","        image_encoding = batch_data['image_enc']\n","        question_encoding = batch_data['ques_enc']\n","        question_length = batch_data['ques_len']\n","        ground_truth_answer = batch_data['ans_enc']\n","\n","        predicted_answer = model(image_encoding, question_encoding)  # for SimpleBaselineNet and Transformer\n","        # predicted_answer = model(image_encoding, question_encoding, question_length)  # only for CoattentionNet \n","        \n","        criterion = torch.nn.CrossEntropyLoss()\n","\n","        # optimizer = torch.optim.SGD([{'params': model.fc_ques.parameters(), 'lr': 0.8}, {'params': model.classifier.parameters(), 'lr': 0.01}])  # for SimpleBaselineNet \n","\n","        # optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)  # for CoattentionNet \n","\n","        optimizer = torch.optim.AdamW([{'params': model.parameters(), 'lr': 0.0005, 'eps': 1e-6, 'weight_decay': 0.000001}])  # for Transformer\n","\n","        loss = optimize(criterion, predicted_answer, optimizer, ground_truth_answer)\n","\n","        if batch_id == (num_batches - 1):\n","            model.eval()  # Set the model to eval mode\n","            val_accuracy = validate(model, val_dataset_loader)\n","            print(\"Epoch {} has val accuracy: {}\".format(epoch, val_accuracy))\n","            out_filename.write(\"Epoch {} has val accuracy: {}\\n\".format(epoch, val_accuracy))\n","\n","\n","out_filename.close() "]},{"cell_type":"markdown","source":["#  Plots and Tables\n"],"metadata":{"id":"_HK3rXIeqf58"}},{"cell_type":"code","source":["figure = plt.figure(figsize=(20, 15))\n","\n","# Simple Baseline Net\n","plt.subplot(3, 3, 1)\n","epoch_list_SimpleBaselineNet = []\n","accuracy_list_SimpleBaselineNet = []\n","\n","with open('/content/drive/MyDrive/EasyVisualQuestionAnswering/SimpleBaselineNet.txt', 'r') as f:\n","    data = f.read().split(\"\\n\")\n","for i in range(len(data)-1):\n","    epoch_list_SimpleBaselineNet.append(int(data[i].split(\" \")[1]))\n","    accuracy_list_SimpleBaselineNet.append(float(data[i].split(\" \")[5]))\n","\n","plt.plot(epoch_list_SimpleBaselineNet, accuracy_list_SimpleBaselineNet, label=\"Simple Baseline Net\", color='blue', linewidth=3.0)\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('Accuracy', fontsize=15)\n","plt.legend(loc='lower right', prop={'size': 15})\n","plt.savefig('/content/drive/MyDrive/EasyVisualQuestionAnswering/SimpleBaselineNet.png')\n","\n","# Parallel Coattention Net\n","plt.subplot(3, 3, 4)\n","epoch_list_ParallelCoattentionNet = []\n","accuracy_list_ParallelCoattentionNet = []\n","\n","with open('/content/drive/MyDrive/EasyVisualQuestionAnswering/ParallelCoattentionNet.txt', 'r') as f:\n","    data = f.read().split(\"\\n\")\n","for i in range(len(data)-1):\n","    epoch_list_ParallelCoattentionNet.append(int(data[i].split(\" \")[1]))\n","    accuracy_list_ParallelCoattentionNet.append(float(data[i].split(\" \")[5]))\n","\n","plt.plot(epoch_list_ParallelCoattentionNet, accuracy_list_ParallelCoattentionNet, label=\"Parallel Coattention Net\", color='green', linewidth=3.0)\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('Accuracy', fontsize=15)\n","plt.legend(loc='lower right', prop={'size': 15})\n","plt.savefig('/content/drive/MyDrive/EasyVisualQuestionAnswering/ParallelCoattentionNet.png')\n","\n","# Alternate Coattention Net\n","plt.subplot(3, 3, 5)\n","epoch_list_AlternateCoattentionNet = []\n","accuracy_list_AlternateCoattentionNet = []\n","\n","with open('/content/drive/MyDrive/EasyVisualQuestionAnswering/AlternateCoattentionNet.txt', 'r') as f:\n","    data = f.read().split(\"\\n\")\n","for i in range(len(data)-1):\n","    epoch_list_AlternateCoattentionNet.append(int(data[i].split(\" \")[1]))\n","    accuracy_list_AlternateCoattentionNet.append(float(data[i].split(\" \")[5]))\n","\n","plt.plot(epoch_list_AlternateCoattentionNet, accuracy_list_AlternateCoattentionNet, label=\"Alternate Coattention Net\", color='green', linewidth=3.0)\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('Accuracy', fontsize=15)\n","plt.legend(loc='lower right', prop={'size': 15})\n","plt.savefig('/content/drive/MyDrive/EasyVisualQuestionAnswering/AlternateCoattentionNet.png')\n","\n","# Encoder Decoder\n","plt.subplot(3, 3, 7)\n","epoch_list_Transformer = []\n","accuracy_list_Transformer = []\n","\n","with open('/content/drive/MyDrive/EasyVisualQuestionAnswering/Transformer.txt', 'r') as f:\n","    data = f.read().split(\"\\n\")\n","for i in range(len(data)-1):\n","    epoch_list_Transformer.append(int(data[i].split(\" \")[1]))\n","    accuracy_list_Transformer.append(float(data[i].split(\" \")[5]))\n","\n","plt.plot(epoch_list_Transformer, accuracy_list_Transformer, label=\"Transformer\", color='red', linewidth=3.0)\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('Accuracy', fontsize=15)\n","plt.legend(loc='lower right', prop={'size': 15})\n","plt.savefig('/content/drive/MyDrive/EasyVisualQuestionAnswering/Transformer.png')"],"metadata":{"id":"icxI2UysObMc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame({\"Net\": [\"Simple Baseline Net\", \"Parallel Coattention Net\", \"Alternate Coattention Net\", \"Transformer\"],\n","                   \"Epochs\": [epoch_list_SimpleBaselineNet[-1]+1, epoch_list_ParallelCoattentionNet[-1]+1, epoch_list_AlternateCoattentionNet[-1]+1, epoch_list_Transformer[-1]+1],\n","                   \"Accuracy\": [accuracy_list_SimpleBaselineNet[-1], accuracy_list_ParallelCoattentionNet[-1], accuracy_list_AlternateCoattentionNet[-1], accuracy_list_Transformer[-1]]})\n","\n","df.to_csv('/content/drive/MyDrive/EasyVisualQuestionAnswering/results.csv', index=False, encoding='utf-8')\n","\n","print(df.to_string(index=False))"],"metadata":{"id":"m-j_sRPF4tvY","executionInfo":{"status":"aborted","timestamp":1663055527518,"user_tz":-120,"elapsed":29,"user":{"displayName":"debora capriotti","userId":"06770130756812580429"}}},"execution_count":null,"outputs":[]}]}